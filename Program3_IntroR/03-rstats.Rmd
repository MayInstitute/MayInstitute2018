---
title: 'Day 3: Beginner''s statistics in R'
author: "Laurent Gatto and Meena Choi"
output:
  html_document:
   contained: true
   toc: true
   toc_float: true
   fig_caption: no
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objectives

- Randomization and basic statistics
- Statistical hypothesis testing: t-test
- Sample size calculation
- Analysis for categorical data
- Linear regression and correlation

---

# Part 1: Basic statistics

## Randomization

### Random selection of samples from a larger set

Let's assume that we have the population with a total of 10 subjects. Suppose we label them from 1 to 10 and randomly would like
to select 3 subjects we can do this using the `sample` function. When
we run `sample` another time, different subjects will be selected. Try
this a couple times.

```{r}
sample(10, 3)
sample(10, 3)
```

Now suppose we would like to select the same randomly selected samples
every time, then we can use a random seed number.

```{r}
set.seed(3728)
sample(10, 3)

set.seed(3728)
sample(10, 3)
```

Let's practice with fun example. Select two in our group member for coming early next Monday.
```{r}
group.member <- c('Cyril', 'Dan', 'Kylie', 'Meena', 'Sara', 'Ting', 'Tsung-Heng', 'Tyler')
sample(group.member, 2)
```


### Completely randomized order of MS runs

Let's load `iprg` data first.
```{r}
load('./data/iprg.rda')
```

We can also create a random order using all elements of iPRG
dataset. Again, we can achieve this using `sample`, asking for exactly
the amount of samples in the subset. This time, each repetition gives
us a different order of the complete set.

```{r}
msrun <- unique(iprg$Run)
msrun

## randomize order among all 12 MS runs
sample(msrun, length(msrun))

## different order will be shown.
sample(msrun, length(msrun))
```

### Randomized block design

- Allow to remove known sources of variability that you are not
  interested in.

- Group conditions into blocks such that the conditions in a block are
  as similar as possible.

- Randomly assign samples with a block.

This particular dataset contains a total of 12 MS runs across 4
conditions, 3 technical replicates per condition. Using the
`block.random` function in the `psych` package, we can achieve
randomized block designs! `block.random` function makes random assignment of `n` subjects with an equal number in all of `N` conditions.

```{r, message = FALSE}
library("psych") ## load the psych package

msrun <- unique(iprg[, c('Condition','Run')])
msrun

## 4 Conditions of 12 MS runs randomly ordered
block.random(n = 12, c(Condition = 4))
block.random(n = 12, c(Condition = 4, BioReplicate=3))

```


## Basic statistical summaries

```{r}
library(dplyr)
```
### Calculate simple statistics

Let's start data with one protein as an example and calculate the
mean, standard deviation, standard error of the mean across all
replicates per condition. We then store all the computed statistics
into a single summary data frame for easy access.

We can use the `aggregate` function to compute summary statistics. `aggregate` splits the data into subsets, computes summary statistics for each, and returns the result in a convenient form.

```{r}
# check what proteins are in dataset, show all protein names
head(unique(iprg$Protein))
length(unique(iprg$Protein))

#distinct(iprg, Protein)
n_distinct(iprg$Protein)
```

```{r}
# Let's start with one protein, named "sp|P44015|VAC2_YEAST"
oneproteindata <- iprg[iprg$Protein == "sp|P44015|VAC2_YEAST", ]

# there are 12 rows in oneproteindata
oneproteindata

# with dplyr
oneproteindata.bcp <- filter(iprg, Protein == "sp|P44015|VAC2_YEAST")
oneproteindata.bcp
```


```{r, eval = FALSE}
# If you want to see more details,
?aggregate
```

### Calculate mean per groups

```{r}
## splits 'oneproteindata' into subsets by 'Condition',
## then, compute 'FUN=mean' of 'log2Int'
sub.mean <- aggregate(Log2Intensity ~ Condition,
					  data = oneproteindata,
					  FUN = mean)
sub.mean

# with dplyr
sub.mean.bcp <- oneproteindata %>% 
    group_by(Condition) %>%
    summarise(mean=mean(Log2Intensity))

sub.mean.bcp
```

### Calculate SD (standard deviation) per groups

$$ s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2} $$

> **Challenge**
>
> Using the `aggregate` function above, calculate the standard
> deviation, by applying the `median` function.

<details>
```{r}
## The same as mean calculation above. 'FUN' is changed to 'sd'.
sub.median <- aggregate(Log2Intensity ~ Condition,
					data = oneproteindata, FUN = median)
sub.median

# with dplyr
sub.median.bcp <- oneproteindata %>% 
    group_by(Condition) %>%
    summarise(median=median(Log2Intensity))

sub.median.bcp
```
</details>

> Using the `aggregate` function above, calculate the standard
> deviation, by applying the `sd` function.

<details>
```{r}
## The same as mean calculation above. 'FUN' is changed to 'sd'.
sub.sd <- aggregate(Log2Intensity ~ Condition,
					data = oneproteindata, FUN = sd)
sub.sd

# with dplyr
sub.sd.bcp <- oneproteindata %>% 
    group_by(Condition) %>%
    summarise(sd = sd(Log2Intensity))

sub.sd.bcp
```
</details>

### Count the number of observation per groups

> **Challenge**
>
> Using the `aggregate` function above, count the number of
> observations per group with the `length` function.

<details>
```{r}
## The same as mean calculation. 'FUN' is changed 'length'.
sub.len <- aggregate(Log2Intensity ~ Condition,
					 data = oneproteindata,
					 FUN = length)
sub.len

# with dplyr
sub.len.bcp <- oneproteindata %>% 
    group_by(Condition) %>%
    summarise(count = n())

sub.len.bcp
```
</details>

### Calculate SE (standard error of mean) per groups

$$ SE = \sqrt{\frac{s^2}{n}} $$

```{r}
sub.se <- sqrt(sub.sd$Log2Intensity^2 / sub.len$Log2Intensity)
sub.se
```

We can now make the summary table including the results above (mean,
sd, se and length).

```{r}
## paste0 : concatenate vectors after convering to character.
(grp <- paste0("Condition", 1:4))
## It is equivalent to paste("Condition", 1:4, sep="")
summaryresult <- data.frame(Group = grp,
							mean = sub.mean$Log2Intensity,
							sd = sub.sd$Log2Intensity,
							se = sub.se,
							length = sub.len$Log2Intensity)
summaryresult
```

> **Challenge**
>
> Make the same table as summaryresult with dplyr package.

<details>
```{r}
summaryresult.dplyr <- oneproteindata %>% 
    group_by(Condition) %>%
    summarise(mean = mean(Log2Intensity),
              sd = sd(Log2Intensity),
              length = n())
summaryresult.dplyr <- mutate(summaryresult.dplyr, se=sqrt(sd^2 / length))

summaryresult.dplyr
```
</details>

## Visualization with error bars for descriptive purpose

*error bars* can have a variety of meanings or conclusions if what
they represent is not precisely specified. Below we provide some
examples of which types of error bars are common. We're using the
summary of protein `sp|P44015|VAC2_YEAST` from the previous section
and the `ggplot2` package as it provides a convenient way to make
easily adaptable plots.

```{r}
library(ggplot2)
```

```{r}
# means without any errorbar
p <- ggplot(aes(x = Group, y = mean, colour = Group),
			data = summaryresult)+
	geom_point(size = 3)
p
```

Let's change a number of visual properties to make the plot more attractive.

* Let's change the labels of x-axis and y-axis and title: `labs(title="Mean", x="Condition", y='Log2(Intensity)')`
* Let's change background color for white: `theme_bw()`
* Let's change size or color of labels of axes and title, text of
  x-axis by using a *theme*
* Let's change the position of legend (use `'none'` to remove it)
* Let's make the box for legend
* Let's remove the box for legend key.

See also this
[post](http://ggplot2.tidyverse.org/reference/theme.html) for options of *theme*, 
[post](http://ggplot2.tidyverse.org/reference/ggtheme.html) for complete theme.

```{r}
p2 <- p + labs(title = "Mean", x = "Group", y = 'Log2(Intensity)') +
	theme_bw() +
	theme(plot.title = element_text(size = 25, colour = "darkblue"),
		  axis.title.x = element_text(size = 15),
		  axis.title.y = element_text(size = 15),
		  axis.text.x = element_text(size = 13),
		  legend.position = 'bottom',
		  legend.background = element_rect(colour = 'black'),
		  legend.key = element_rect(colour = 'white'))
p2
```

Let's now add the **standard deviation**:

```{r}
# mean with SD
p2 + geom_errorbar(aes(ymax = mean + sd, ymin = mean - sd), width = 0.1) +
	  labs(title="Mean with SD")
```

> **Challenge**
>
> Add the **standard error of the mean**. Which one is smaller?

<details>
```{r}
# mean with SE
p2 + geom_errorbar(aes(ymax = mean + se, ymin=mean - se), width = 0.1) +
	labs(title="Mean with SE")
## The SE is narrow than the SD!
```
</details>

> **Challenge**
>
> Add the **standard error of the mean** with black color.

<details>
```{r}
# mean with SE
p2 + geom_errorbar(aes(ymax = mean + se, ymin=mean - se), width = 0.1, color='black') +
	labs(title="Mean with SE")
```
</details>

## Working with statistical distributions

For each statistical distribution, we have function to compute

* density
* distribution function
* quantile function
* random generation

For the normale distribution `norm`, these are respectively

* `dnorm`
* `pnorm`
* `qnorm`
* `rnorm`

Let's start by sampling 1000000 values from a normal distribution $N(0, 1)$:

```{r}
xn <- rnorm(1e6)
hist(xn, freq = FALSE)
rug(xn)
lines(density(xn), lwd = 2)
```

By definition, the area under the density curve is 1. The area at the
left of 0, 1, and 2 are respectively:

```{r}
pnorm(0)
pnorm(1)
pnorm(2)
```

To ask the inverse question, we use the quantile function. The obtain
0.5, `r pnorm(1)` and `r pnorm(2)` of our distribution, we need means
of:

```{r}
qnorm(0.5)
qnorm(pnorm(1))
qnorm(pnorm(2))
qnorm(0.05)
```

Finally, the density function gives us the *height* at which we are
for a given mean:

```{r}
hist(xn, freq = FALSE)
lines(density(xn), lwd = 2)
points(0, dnorm(0), pch = 19, col = "red")
points(1, dnorm(1), pch = 1, col = "blue")
points(2, dnorm(2), pch = 4, col = "orange")
```

## Calculate the confidence interval

Now that we've covered the standard error of the mean and the standard
deviation, let's investigate how we can add custom confidence
intervals (CI) for our measurement of the mean. We'll add these CI's
to the summary results we previously stored for protein
`sp|P44015|VAC2_YEAST`.

Confidence interval:

$$\mbox{mean} \pm (SE \times \frac{\alpha}{2} ~ \mbox{quantile of t distribution})$$


To calculate the 95% confident interval, we need to be careful and set
the quantile for two-sided. We need to divide by two for error.  For
example, 95% confidence interval, right tail is 2.5% and left tail is
2.5%.


```{r}
summaryresult$ciw.lower.95 <- summaryresult$mean -
	qt(0.975, summaryresult$len - 1) * summaryresult$se
summaryresult$ciw.upper.95 <- summaryresult$mean +
	qt(0.975, summaryresult$len - 1) * summaryresult$se
summaryresult
```
<details>
```{r}
summaryresult.dplyr %>% mutate(ciw.lower.95 = mean - qt(0.975, length-1)*se,
                               ciw.upper.95 = mean + qt(0.975, length-1)*se)
summaryresult.dplyr
```
</details>

```{r}
# mean with 95% two-sided confidence interval
ggplot(aes(x = Group, y = mean, colour = Group),
	   data = summaryresult) +
	geom_point() +
	geom_errorbar(aes(ymax = ciw.upper.95, ymin = ciw.lower.95), width = 0.1) +
	labs(title="Mean with 95% confidence interval", x="Condition", y='Log2(Intensity)') +
	theme_bw() +
	theme(plot.title = element_text(size=25, colour="darkblue"),
		  axis.title.x = element_text(size=15),
		  axis.title.y = element_text(size=15),
		  axis.text.x = element_text(size=13),
		  legend.position = 'bottom',
		  legend.background = element_rect(colour = 'black'),
		  legend.key = element_rect(colour='white'))
```

> **Challenges**
>
> Replicate the above for the 99% two-sided confidence interval.

<details>
```{r}
# mean with 99% two-sided confidence interval
summaryresult$ciw.lower.99 <- summaryresult$mean - qt(0.995,summaryresult$len-1) * summaryresult$se
summaryresult$ciw.upper.99 <- summaryresult$mean + qt(0.995,summaryresult$len-1) * summaryresult$se
summaryresult

ggplot(aes(x = Group, y = mean, colour = Group),
	   data = summaryresult) +
	geom_point() +
	geom_errorbar(aes(ymax = ciw.upper.99, ymin=ciw.lower.99), width=0.1) +
	labs(title="Mean with 99% confidence interval", x="Condition", y='Log2(Intensity)') +
	theme_bw()+
	theme(plot.title = element_text(size=25, colour="darkblue"),
		  axis.title.x = element_text(size=15),
		  axis.title.y = element_text(size=15),
		  axis.text.x = element_text(size=13),
		  legend.position = 'bottom',
		  legend.background = element_rect(colour='black'),
		  legend.key = element_rect(colour='white'))
```
</details>

### Some comments

* Error bars with SD and CI are overlapping between groups!

* Error bars for the SD show the spread of the population while error
  bars based on SE reflect the uncertainty in the mean and depend on
  the sample size.

* Confidence intervals of `n` on the other hand mean that the
  intervals capture the population mean `n` percent of the time.

* When the sample size increases, CI and SE are getting closer to each
  other.

## Saving our results

We have two objects that contain all the information that we have
generated so far:

* The `summaryresults` and `summaryresults.dplyr` objects, that contains all the summary
  statistics.


```{r}
save(summaryresult, file = "./data/summaryresults.rda")
save(summaryresult.dplyr, file = "./data/summaryresults.dplyr.rda")

```

We can also save the summary result as a `csv` file using the
`write.csv` function:

```{r, eval=FALSE}
write.csv(sumamryresult, file = "./data/summary.csv")
```

**Tip**: Exporting to csv is useful to share your work with
collaborators that do not use R, but for many continous work in R, to
assure data validity accords platforms, the best format is `rda`.

***
# Part 2: Statistical hypothesis test

First, we are going to prepare the session for further analyses.

```{r}
load("./data/summaryresults.rda")
load("./data/iprg.rda")
```

## Two sample t-test for one protein with one feature

Now, we'll perform a t-test whether protein `sp|P44015|VAC2_YEAST` has
a change in abundance between Condition 1 and Condition 2.

### Hypothesis

* $H_0$: no change in abundance, mean(Condition1) - mean(Condition2) = 0
* $H_a$: change in abundance, mean(Condition1) - mean(Condition 2) $\neq$ 0

### Statistics

* Observed $t = \frac{\mbox{difference of group means}}{\mbox{estimate of variation}} = \frac{(mean_{1} - mean_{2})}{SE} \sim t_{\alpha/2, df}$
* Standard error, $SE=\sqrt{\frac{s_{1}^2}{n_{1}} + \frac{s_{2}^2}{n_{2}}}$

with

* $n_{i}$: number of replicates
* $s_{i}^2 = \frac{1}{n_{i}-1} \sum (Y_{ij} - \bar{Y_{i \cdot}})^2$: sample variance

### Data preparation

```{r}
## Let's start with one protein, named "sp|P44015|VAC2_YEAST"
oneproteindata <- iprg[iprg$Protein == "sp|P44015|VAC2_YEAST", ]

## Then, get two conditions only, because t.test only works for two groups (conditions).
oneproteindata.condition12 <- oneproteindata[oneproteindata$Condition %in%
											 c('Condition1', 'Condition2'), ]
oneproteindata.condition12
table(oneproteindata.condition12[, c("Condition", "BioReplicate")])
```

<details>
```{r}
## with dplyr
## Let's start with one protein, named "sp|P44015|VAC2_YEAST"
oneproteindata <- filter(iprg, Protein == "sp|P44015|VAC2_YEAST")

## Then, get two conditions only, because t.test only works for two groups (conditions).
oneproteindata.subset <- filter(oneproteindata, 
                                Condition %in% c('Condition1', 'Condition2'))
oneproteindata.subset
table(oneproteindata.subset[, c("Condition", "BioReplicate")])
```
</details>

If we want to remove the levels that are not relevant anymore, we can
use `droplevels`:

```{r}
oneproteindata.subset <- droplevels(oneproteindata.subset)
table(oneproteindata.subset[, c("Condition", "BioReplicate")])
```

To perform the t-test, we use the `t.test` function. Let's first
familiarise ourselves with it by looking that the manual

```{r, eval=FALSE}
?t.test
```

And now apply to to our data

```{r}
# t test for different abundance (log2Int) between Groups (Condition)
result <- t.test(Log2Intensity ~ Condition,
				 data = oneproteindata.subset,
				 var.equal = FALSE)

result
```

> **Challenge**
>
> Repeat the t-test above but with calculating a 90% confidence interval
> for the log2 fold change.

<details>
```{r, eval=FALSE, echo=FALSE}
result.ci90 <- t.test(Log2Intensity ~ Condition,
					  var.equal = FALSE,
					  data = oneproteindata.condition12,
					  conf.level = 0.9)
result.ci90
```
</details>

### The `htest` class

The `t.test` function, like other hypothesis testing function, return
a result of a type we haven't encountered yet, the `htest` class:

```{r}
class(result)
```

which stores typical results from such tests. Let's have a more
detailed look at what information we can learn from the results our
t-test. When we type the name of our `result` object, we get a short
textual summary, but the object contains more details:

```{r}
names(result)
```

and we can access each of these by using the `$` operator, like we
used to access a single column from a `data.frame`, but the `htest`
class is not a `data.frame` (it's actually a `list`). For example, to
access the group means, we would use

```{r}
result$estimate
```

> **Challenge**
>
> * Calculate the (log2-transformed) fold change between groups
> * Extract the value of the t-statistics
> * Calculate the standard error (fold-change/t-statistics)
> * Extract the degrees of freedom (parameter)
> * Extract the p values
> * Extract the 95% confidence intervals

<details>
```{r, echo=FALSE, include=FALSE}
## log2 fold-change
result$estimate[1]-result$estimate[2]
## test statistic value, T value
result$statistic
## standard error
(result$estimate[1]-result$estimate[2])/result$statistic
## degree of freedom
result$parameter
## p value for two-sides testing
result$p.value
## 95% confidence interval for log2 fold change
result$conf.int
```
</details>

We can also manually compute our t-test statistic using the formulas
we descibed above and compare it with the `summaryresult`.

Recall the `summaryresult` we generated last section.

```{r}
summaryresult
summaryresult12 <- summaryresult[1:2, ]

## test statistic, It is the same as 'result$statistic' above.
diff(summaryresult12$mean) ## different sign, but absolute values are same as result$estimate[1]-result$estimate[2]
sqrt(sum(summaryresult12$sd^2/summaryresult12$length)) ## same as stand error

## the t-statistic : sign is different
diff(summaryresult12$mean)/sqrt(sum(summaryresult12$sd^2/summaryresult12$length))
```

## Re-calculating the p values

Referring back to our t-test results above, we can manually calculate
the one- and two-side tests p-values using the t-statistics and the
test parameter (using the `pt` function).


Our result t statistic was `r as.vector(result$statistic)` (accessible
with `result$statistic`). Let's start by visualising it along a t
distribution. Let's create data from such a distribution, making sure
we set to appropriate parameter.

```{r}
## generate 10^5 number with the same degree of freedom for distribution.
xt <- rt(1e5, result$parameter)
plot(density(xt), xlim = c(-10, 10))
abline(v = result$statistic, col = "red") ## where t statistics are located.
abline(h = 0, col = "gray") ## horizontal line at 0
```

**The area on the left** of that point is given by `pt(result$statistic,
result$parameter)`, which is `r pt(result$statistic,
result$parameter)`. The p-value for a one-sided test, which is ** the area on the right** of red line, is this given by

```{r}
1 - pt(result$statistic, result$parameter)
```

And the p-value for a two-sided test is

```{r}
2 * (1 - pt(result$statistic, result$parameter))
```

which is the same as the one calculated by the t-test.


***
# Part 3 : Choosing a model

The decision of which statistical model is appropriate for a given set of observations depends on the type of data that have been collected.

* Quantitative response with quantitative predictors : regression model

* Categorical response with quantitative predictors : logistic regression model for bivariate categorical response (e.g., Yes/No, dead/alive), multivariate logistic regression model when the response variable has more than two possible values.

* Quantitative response with categorical predictors : ANOVA model (quantitative response across several populations defined by one or more categorical predictor variables)

* Categorical response with categorical predictors : contingency table that can be used to draw conclusions about the relationships between variables.


Part 5b are adapted from *Bremer & Doerge*,
[Using R at the Bench : Step-by-Step Data Analytics for Biologists](https://www.amazon.com/dp/1621821129/ref=olp_product_details?_encoding=UTF8&me=)
, cold Spring Harbor LaboratoryPress, 2015.


***


# Part 4: Sample size calculation

To calculate the required sample size, you’ll need to know four
things:

* $\alpha$: confidence level
* $power$: 1 - $\beta$, where $\beta$ is probability of a true positive discovery
* $\Delta$: anticipated fold change
* $\sigma$: anticipated variance

## R code

Assuming equal varaince and number of samples across groups, the
following formula is used for sample size estimation:

$$\frac{2{\sigma}^2}{n}\leq(\frac{\Delta}{z_{1-\beta}+z_{1-\alpha/2}})^2$$


```{r}
library("pwr")

## ?pwr.t.test

# Significance level alpha
alpha <- 0.05

# Power = 1 - beta
power <- 0.95

# anticipated log2 fold change
delta <- 1

# anticipated variability
sigma <- 0.9

# Effect size
# It quantifies the size of the difference between two groups
d <- delta/sigma

#Sample size estimation
pwr.t.test(d = d, sig.level = alpha, power = power, type = 'two.sample')
```


> **Challenge**
>
> * Calculate power with 10 samples and the same parameters as above.

<details>
```{r, echo=FALSE, include=FALSE}
## log2 fold-change
pwr.t.test(d = d, sig.level = alpha, n=10, power = NULL, type = 'two.sample')
```
</details>

Let's investigate the effect of required fold change and variance on the sample size estimation.

```{r, warning=FALSE}
# anticipated log2 fold change
delta <- seq(0.1, 0.7, .1)
nd <- length(delta)

# anticipated variability
sigma <- seq(0.1,0.5,.1)
ns <- length(sigma)

# obtain sample sizes
samsize <- matrix(0, nrow=ns*nd, ncol = 3)
counter <- 0
for (i in 1:nd){
  for (j in 1:ns){
	result <- pwr.t.test(d = delta[i] / sigma[j],
						 sig.level = alpha,
						 power = power,
						 type = "two.sample")
	counter <- counter + 1
	samsize[counter, 1] <- delta[i]
	samsize[counter, 2] <- sigma[j]
	samsize[counter, 3] <- ceiling(result$n)
  }
}
colnames(samsize) <- c("desiredlog2FC", "variability", "samplesize")

## visualization
samsize <- as.data.frame(samsize)
samsize$variability <- as.factor(samsize$variability)
ggplot(data=samsize, aes(x=desiredlog2FC, y=samplesize, group = variability, colour = variability)) +
  geom_line() +
  geom_point(size=2, shape=21, fill="white") +
  labs(title="Significance level=0.05, Power=0.95", x="Anticipated log2 fold change", y='Sample Size (n)') +
  theme(plot.title = element_text(size=20, colour="darkblue"),
		axis.title.x = element_text(size=15),
		axis.title.y = element_text(size=15),
		axis.text.x = element_text(size=13))
```

***

# Part 5: Linear models and correlation

When considering correlations and modelling data, visualization is key.

Let's use the famous
[*Anscombe's quartet*](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)
data as a motivating example. This data is composed of 4 pairs of
values, $(x_1, y_1)$ to $(x_4, y_4)$:

```{r anscombe, echo = FALSE, results='asis'}
library("knitr")
kable(anscombe)
```

Each of these $x$ and $y$ sets have the same variance, mean and
correlation:

```{r anscombetab, echo=FALSE}
tab <- matrix(NA, 5, 4)
colnames(tab) <- 1:4
rownames(tab) <- c("var(x)", "mean(x)",
				   "var(y)", "mean(y)",
				   "cor(x,y)")

for (i in 1:4)
	tab[, i] <- c(var(anscombe[, i]),
				  mean(anscombe[, i]),
				  var(anscombe[, i+4]),
				  mean(anscombe[, i+4]),
				  cor(anscombe[, i], anscombe[, i+4]))

```

```{r anstabdisplay, echo=FALSE}
kable(tab)
```

But...

While the *residuals* of the linear regression clearly indicate
fundamental differences in these data, the most simple and
straightforward approach is *visualisation* to highlight the
fundamental differences in the datasets.

```{r anscombefig, echo=FALSE}
ff <- y ~ x

mods <- setNames(as.list(1:4), paste0("lm", 1:4))

par(mfrow = c(2, 2), mar = c(4, 4, 1, 1))
for (i in 1:4) {
	ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
	plot(ff, data = anscombe, pch = 19, xlim = c(3, 19), ylim = c(3, 13))
	mods[[i]] <- lm(ff, data = anscombe)
	abline(mods[[i]])
}
```

See also another, more recent example:
[The Datasaurus Dozen dataset](https://www.autodeskresearch.com/publications/samestats).
<details>

![The Datasaurus Dozen dataset](https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif)
</details>


## Correlation

Here is an example where a wide format comes very handy. We are going
to convert our iPRG data using the `spread` function from the `tidyr`
package:


```{r}
library("tidyr")
iprg2 <- spread(iprg[, 1:3], Run, Log2Intensity)
rownames(iprg2) <- iprg2$Protein
iprg2 <- iprg2[, -1]
```

```{r, echo = FALSE}
if (!file.exists("./data/iprg2.rda"))
	save(iprg2, file = "./data/iprg2.rda")
```

And lets focus on the 3 runs, i.e. 2 replicates from condition
1 and 1 replicate from condition 4.

```{r}
x <- iprg2[, c(1, 2, 10)]
head(x)
pairs(x)
```

We can use the `cor` function to calculate the Pearson correlation
between two vectors of the same length (making sure the order is
correct), or a dataframe. But, we have missing values in the data,
which will stop us from calculating the correlation:

```{r}
cor(x)
```

We first need to omit the proteins/rows that contain missing values

```{r}
x2 <- na.omit(x)
cor(x2)
```

### A note on correlation and replication

It is often assumed that high correlation is a halmark of good
replication. Rather than focus on the correlation of the data, a
better measurement would be to look a the log2 fold-changes, i.e. the
distance between or repeated measurements. The ideal way to visualise
this is on an MA-plot:


```{r, fig.width = 12}
par(mfrow = c(1, 2))
r1 <- x2[, 1]
r2 <- x2[, 2]
M <- r1 - r2
A <- (r1 + r2)/2
plot(A, M); grid()
suppressPackageStartupMessages(library("affy"))
affy::ma.plot(A, M)
```

See also this
[post](http://simplystatistics.org/2015/08/12/correlation-is-not-a-measure-of-reproducibility/)
on the *Simply Statistics* blog.

## Linear modelling

`abline(0, 1)` can be used to add a line with intercept 0 and
slop 1. It we want to add the line that models the data linearly, we
can calculate the parameters using the `lm` function:

```{r}
lmod <- lm(r2 ~ r1)
summary(lmod)
```

which can be used to add the adequate line that reflects the (linear)
relationship between the two data

```{r}
plot(r1, r2)
abline(lmod, col = "red")
```

As we have seen in the beginning of this section, it is essential not
to rely solely on the correlation value, but look at the data. This
also holds true for linear (or any) modelling, which can be done by
plotting the model:

```{r}
par(mfrow = c(2, 2))
plot(lmod)
```

* *Cook's distance* is a commonly used estimate of the influence of a
  data point when performing a least-squares regression analysis and
  can be used to highlight points that particularly influence the
  regression.

* *Leverage* quantifies the influence of a given observation on the
  regression due to its location in the space of the inputs.

See also `?influence.measures`.


> **Challenge**
>
> 1. Take any of the `iprg2` replicates, model and plot their linear
>    relationship. The `iprg2` data is available as an `rda` file, or
>    regenerate it as shown above.
> 2. The Anscombe quartet is available as `anscombe`. Load it, create
>    a linear model for one $(x_i, y_i)$ pair of your choice and
>    visualise/check the model.

<details>
```{r}
x3 <- anscombe[, 3]
y3 <- anscombe[, 7]
lmod <- lm(y3 ~ x3)
summary(lmod)
par(mfrow = c(2, 2))
plot(lmod)
```
</details>

Finally, let's conclude by illustrating how `ggplot2` can very
elegantly be used to produce similar plots, with useful annotations:

```{r, message=FALSE}
library("ggplot2")
dfr <- data.frame(r1, r2, M, A)
p <- ggplot(aes(x = r1, y = r2), data = dfr) + geom_point()
p + geom_smooth(method = "lm") +
	geom_quantile(colour = "red")
```

> **Challenge**
>
> Replicate the MA plot above using `ggplot2`. Then add a
> non-parametric lowess regression using `geom_smooth()`.

<details>
```{r}
p <- ggplot(aes(x = A, y = M), data = dfr) + geom_point()
p + geom_smooth() + geom_quantile(colour = "red")
```
</details>


---

Back to course [home page](http://bit.ly/2018MayInstRstats)
